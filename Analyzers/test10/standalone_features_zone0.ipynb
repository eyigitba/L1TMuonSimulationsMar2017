{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "  print('[ERROR] You need to run this with Python 3.')\n",
    "  raise AssertionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import toolz\n",
    "\n",
    "from emtf_algos import *\n",
    "from emtf_logger import get_logger\n",
    "from emtf_colormap import get_colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Using cmssw      : CMSSW_10_6_3\n",
      "[INFO    ] Using python     : 3.6.10 |Anaconda, Inc.| (default, Mar 25 2020, 23:51:54) [GCC 7.3.0]\n",
      "[INFO    ] Using numpy      : 1.19.2\n",
      "[INFO    ] Using tensorflow : 2.4.0\n",
      "[INFO    ] Using keras      : 2.4.0\n",
      "[INFO    ] .. list devices  : [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[INFO    ] Using matplotlib : 3.3.2\n",
      "[INFO    ] Using dask       : 2020.12.0\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(2027)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers as k_layers\n",
    "from tensorflow.keras import backend as k_backend\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(2027)\n",
    "\n",
    "#import numba\n",
    "#from numba import njit, vectorize\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "logger = get_logger()\n",
    "logger.info('Using cmssw      : {0}'.format(os.environ['CMSSW_VERSION'] if 'CMSSW_VERSION' in os.environ else 'n/a'))\n",
    "logger.info('Using python     : {0}'.format(sys.version.replace('\\n', '')))\n",
    "logger.info('Using numpy      : {0}'.format(np.__version__))\n",
    "logger.info('Using tensorflow : {0}'.format(tf.__version__))\n",
    "logger.info('Using keras      : {0}'.format(keras.__version__))\n",
    "logger.info('.. list devices  : {0}'.format(tf.config.list_physical_devices()))\n",
    "logger.info('Using matplotlib : {0}'.format(mpl.__version__))\n",
    "#logger.info('Using numba      : {0}'.format(numba.__version__))\n",
    "logger.info('Using dask       : {0}'.format(dask.__version__))\n",
    "\n",
    "assert k_backend.backend() == 'tensorflow'\n",
    "assert k_backend.image_data_format() == 'channels_last'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Processing zone 0 timezone 0\n",
      "[INFO    ] .. maxevents        : -1\n",
      "[INFO    ] .. workers          : 8\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "\n",
    "# zone: (0,1,2) -> eta=(1.98..2.5, 1.55..1.98, 1.2..1.55)\n",
    "zone = 0\n",
    "#zone = 1\n",
    "#zone = 2\n",
    "\n",
    "# timezone: (0,1,2) -> BX=(0,-1,-2)\n",
    "timezone = 0\n",
    "\n",
    "# masked array filling value\n",
    "ma_fill_value = 999999\n",
    "\n",
    "#maxevents = 10\n",
    "maxevents = -1\n",
    "\n",
    "#workers = 1\n",
    "workers = 8\n",
    "\n",
    "# Input files\n",
    "patterns_fname = 'patterns_zone%i.npz' % zone\n",
    "zone_images_fname = 'zone_images_zone%i.h5' % zone\n",
    "\n",
    "# Scheduler\n",
    "dask.config.set(scheduler='threads', num_workers=workers)\n",
    "\n",
    "# Styling\n",
    "plt.style.use('tdrstyle.mplstyle')\n",
    "cm = get_colormap()\n",
    "\n",
    "logger.info('Processing zone {0} timezone {1}'.format(zone, timezone))\n",
    "logger.info('.. maxevents        : {0}'.format(maxevents))\n",
    "logger.info('.. workers          : {0}'.format(workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "file_handles = []\n",
    "\n",
    "def load_patterns():\n",
    "  patterns = []\n",
    "  boxes_act = []\n",
    "  hitmap_quality_ranks = []\n",
    "  for i in range(num_emtf_zones):\n",
    "    fname = patterns_fname.replace('zone%i' % zone, 'zone%i' % i)  # modify filename\n",
    "    logger.info('Loading from {0}'.format(fname))\n",
    "    with np.load(fname) as loaded:\n",
    "      patterns.append(loaded['patterns'])\n",
    "      boxes_act.append(loaded['boxes_act'])\n",
    "      hitmap_quality_ranks.append(loaded['hitmap_quality_ranks'])\n",
    "  patterns = np.asarray(patterns)\n",
    "  boxes_act = np.asarray(boxes_act)\n",
    "  hitmap_quality_ranks = np.asarray(hitmap_quality_ranks)\n",
    "  logger.info('patterns: {0} boxes_act: {1} hitmap_quality_ranks: {2}'.format(patterns.shape, boxes_act.shape, hitmap_quality_ranks.shape))\n",
    "  return patterns, boxes_act, hitmap_quality_ranks\n",
    "\n",
    "def load_zone_sparse_images(fname):\n",
    "  logger.info('Loading from {0}'.format(fname))\n",
    "  loaded = h5py.File(fname, 'r')\n",
    "  file_handles.append(loaded)\n",
    "  zone_box_anchors = loaded['zone_box_anchors']\n",
    "  zone_sparse_images = SparseTensorValue(indices=loaded['zone_sparse_images_indices'],\n",
    "                                         values=loaded['zone_sparse_images_values'],\n",
    "                                         dense_shape=loaded['zone_sparse_images_dense_shape'])\n",
    "  logger.info('zone_box_anchors: {0} zone_sparse_images: {1}'.format(zone_box_anchors.shape, zone_sparse_images.dense_shape))\n",
    "  return zone_box_anchors, zone_sparse_images\n",
    "\n",
    "def load_zone_hits(fname):\n",
    "  logger.info('Loading from {0}'.format(fname))\n",
    "  loaded = h5py.File(fname, 'r')\n",
    "  file_handles.append(loaded)\n",
    "  zone_part = loaded['zone_part']\n",
    "  zone_hits = RaggedTensorValue(values=loaded['zone_hits_values'],\n",
    "                                row_splits=loaded['zone_hits_row_splits'])\n",
    "  zone_simhits = RaggedTensorValue(values=loaded['zone_simhits_values'],\n",
    "                                   row_splits=loaded['zone_simhits_row_splits'])\n",
    "  logger.info('zone_part: {0} zone_hits: {1} zone_simhits: {2}'.format(zone_part.shape, zone_hits.shape, zone_simhits.shape))\n",
    "  return zone_part, zone_hits, zone_simhits\n",
    "\n",
    "def load_zone_hits_lazy(fname):\n",
    "  logger.info('Loading from {0}'.format(fname))\n",
    "  loaded = h5py.File(fname, 'r')\n",
    "  file_handles.append(loaded)\n",
    "  zone_part = da.from_array(loaded['zone_part'])\n",
    "  zone_hits_values = da.from_array(loaded['zone_hits_values'])\n",
    "  zone_hits_row_splits = da.from_array(loaded['zone_hits_row_splits'])\n",
    "  zone_hits_shape = (zone_hits_row_splits.shape[0] - 1,) + (None,) + zone_hits_values.shape[1:]\n",
    "  zone_simhits_values = da.from_array(loaded['zone_simhits_values'])\n",
    "  zone_simhits_row_splits = da.from_array(loaded['zone_simhits_row_splits'])\n",
    "  zone_simhits_shape = (zone_simhits_row_splits.shape[0] - 1,) + (None,) + zone_simhits_values.shape[1:]\n",
    "  logger.info('zone_part: {0} zone_hits: {1} zone_simhits: {2}'.format(zone_part.shape, zone_hits_shape, zone_simhits_shape))\n",
    "  return zone_part, (zone_hits_values, zone_hits_row_splits), (zone_simhits_values, zone_simhits_row_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Loading from patterns_zone0.npz\n",
      "[INFO    ] Loading from patterns_zone1.npz\n",
      "[INFO    ] Loading from patterns_zone2.npz\n",
      "[INFO    ] patterns: (3, 7, 7, 8, 3) boxes_act: (3, 7, 7, 8, 111, 1) hitmap_quality_ranks: (3, 256)\n",
      "[INFO    ] patterns_reshaped: (3, 7, 8, 3)\n",
      "[INFO    ] boxes_act_reshaped: (3, 1, 111, 8, 7)\n",
      "[INFO    ] boxes_qual_reshaped: (3, 256)\n"
     ]
    }
   ],
   "source": [
    "# Load patterns\n",
    "patterns, boxes_act, hitmap_quality_ranks = load_patterns()\n",
    "\n",
    "# Create patterns_reshaped\n",
    "patterns_reshaped = []\n",
    "for i in range(num_emtf_zones):\n",
    "  p = patterns[i, 3, [3, 2, 4, 1, 5, 0, 6]]  # order by straightness, only prompt patterns\n",
    "  patterns_reshaped.append(p)\n",
    "patterns_reshaped = np.asarray(patterns_reshaped)\n",
    "logger.info('patterns_reshaped: {0}'.format(patterns_reshaped.shape))\n",
    "\n",
    "# Create boxes_act_reshaped\n",
    "boxes_act_reshaped = []\n",
    "for i in range(num_emtf_zones):\n",
    "  b = boxes_act[i, 3, [3, 2, 4, 1, 5, 0, 6]]  # order by straightness, only prompt patterns\n",
    "  b = np.transpose(b, [3, 2, 1, 0])  # kernel shape is HWCD\n",
    "  boxes_act_reshaped.append(b)\n",
    "boxes_act_reshaped = np.asarray(boxes_act_reshaped)\n",
    "logger.info('boxes_act_reshaped: {0}'.format(boxes_act_reshaped.shape))\n",
    "\n",
    "# Create boxes_qual_reshaped\n",
    "boxes_qual_reshaped = hitmap_quality_ranks // 4  # from 8-bit to 6-bit\n",
    "assert boxes_qual_reshaped.max() == 63\n",
    "logger.info('boxes_qual_reshaped: {0}'.format(boxes_qual_reshaped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Loading from zone_images_zone0.h5\n",
      "[INFO    ] zone_part: (652591, 9) zone_hits: (652591, None, 18) zone_simhits: (652591, None, 18)\n"
     ]
    }
   ],
   "source": [
    "# Load zone_hits (lazily)\n",
    "zone_part_l, zone_hits_l, zone_simhits_l = load_zone_hits_lazy(zone_images_fname)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zone_box_anchors, zone_sparse_images = load_zone_sparse_images(zone_images_fname)\n",
    "zone_box_anchors_test = zone_box_anchors[:maxevents]\n",
    "\n",
    "if maxevents != -1 and maxevents < 10000:\n",
    "  zone_images_test = sparse_to_dense_n(zone_sparse_images, maxevents)\n",
    "else:\n",
    "  zone_images_test = sparse_to_dense(zone_sparse_images)[:maxevents]\n",
    "\n",
    "logger.info('zone_box_anchors_test: {0} zone_images_test: {1}'.format(zone_box_anchors_test.shape, zone_images_test.shape))\n",
    "\n",
    "zone_part, zone_hits, zone_simhits = load_zone_hits(zone_images_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data columns\n",
    "hits_metadata = ['emtf_site', 'emtf_host', 'emtf_chamber',\n",
    "                 'emtf_segment', 'zones', 'timezones',\n",
    "                 'emtf_phi', 'emtf_bend', 'emtf_theta',\n",
    "                 'emtf_theta_alt', 'emtf_qual', 'emtf_qual_alt',\n",
    "                 'emtf_time', 'strip', 'wire',\n",
    "                 'fr', 'detlayer', 'bx']\n",
    "hits_metadata = dict(zip(hits_metadata, range(len(hits_metadata))))\n",
    "#print(hits_metadata)\n",
    "\n",
    "# Image format\n",
    "num_channels = 1\n",
    "num_cols = 288  # 80 degrees\n",
    "num_rows = 8\n",
    "image_format = (num_rows, num_cols, num_channels)\n",
    "\n",
    "# Array indices\n",
    "ind_emtf_chamber = hits_metadata['emtf_chamber']\n",
    "ind_emtf_segment = hits_metadata['emtf_segment']\n",
    "\n",
    "ind_emtf_phi = hits_metadata['emtf_phi']\n",
    "ind_emtf_bend = hits_metadata['emtf_bend']\n",
    "ind_emtf_theta1 = hits_metadata['emtf_theta']\n",
    "ind_emtf_theta2 = hits_metadata['emtf_theta_alt']\n",
    "ind_emtf_qual1 = hits_metadata['emtf_qual']\n",
    "ind_emtf_qual2 = hits_metadata['emtf_qual_alt']\n",
    "ind_emtf_time = hits_metadata['emtf_time']\n",
    "ind_zones = hits_metadata['zones']\n",
    "ind_tzones = hits_metadata['timezones']\n",
    "ind_fr = hits_metadata['fr']\n",
    "ind_dl = hits_metadata['detlayer']\n",
    "ind_bx = hits_metadata['bx']\n",
    "ind_valid = hits_metadata['bx']  # CUIDADO: use 'bx' for the moment\n",
    "\n",
    "new_hits_metadata = ['emtf_phi', 'emtf_bend', 'emtf_theta1',\n",
    "                     'emtf_theta2', 'emtf_qual1', 'emtf_qual2',\n",
    "                     'emtf_time', 'zones', 'tzones',\n",
    "                     'fr', 'dl', 'bx',\n",
    "                     'valid']\n",
    "new_hits_metadata = dict(zip(new_hits_metadata, range(len(new_hits_metadata))))\n",
    "#print(new_hits_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaggedTensorLazyValue(object):\n",
    "  def __init__(self, values, row_splits):\n",
    "    _row_splits = np.array(row_splits)  # lazy no more\n",
    "    _values = np.array(values[:_row_splits[-1]])  # lazy no more\n",
    "    self.values = _values\n",
    "    self.row_splits = _row_splits\n",
    "    self.shape = (self.row_splits.shape[0] - 1,) + (None,) + self.values.shape[1:]\n",
    "\n",
    "  def __getitem__(self, row_key):\n",
    "    starts = self.row_splits[:-1]\n",
    "    limits = self.row_splits[1:]\n",
    "    row = self.values[starts[row_key]:limits[row_key]]\n",
    "    return row\n",
    "\n",
    "@dask.delayed\n",
    "def build_inputs(ievt):\n",
    "  zone_hits_ievt = zone_hits[ievt]\n",
    "  zone_hits_columns = [ind_emtf_chamber, ind_emtf_segment]\n",
    "  indices = zone_hits_ievt[:, zone_hits_columns]\n",
    "  zone_hits_columns = [ind_emtf_phi, ind_emtf_bend, ind_emtf_theta1, ind_emtf_theta2,\n",
    "                       ind_emtf_qual1, ind_emtf_qual2, ind_emtf_time, ind_zones,\n",
    "                       ind_tzones, ind_fr, ind_dl, ind_bx,\n",
    "                       ind_valid]\n",
    "  values = zone_hits_ievt[:, zone_hits_columns]\n",
    "  values[:, -1] = 1  # CUIDADO: set 'valid' to 1\n",
    "\n",
    "  # Apply truncation\n",
    "  valid = indices[:, 1] < num_emtf_segments\n",
    "  indices = indices[valid]\n",
    "  values = values[valid]\n",
    "\n",
    "  # Sparse -> Dense\n",
    "  dense_shape = np.array([num_emtf_chambers, num_emtf_segments, num_emtf_variables], dtype=np.int32)\n",
    "  sparse = SparseTensorValue(indices=indices, values=values, dense_shape=dense_shape)\n",
    "  dense = sparse_to_dense(sparse)\n",
    "\n",
    "  # Result\n",
    "  result = (dense, np.concatenate((indices, values), axis=-1))\n",
    "  return result\n",
    "\n",
    "def build_inputs_batch(batch_ids):\n",
    "  results = map(build_inputs, batch_ids)\n",
    "  return results\n",
    "\n",
    "def build_inputs_all():\n",
    "  # Split into batches\n",
    "  batches = np.array_split(np.arange(zone_hits.shape[0]), workers)\n",
    "  results, = dask.compute(map(build_inputs_batch, batches))  # delayed no more\n",
    "\n",
    "  # Chain and zip results\n",
    "  results = list(itertools.chain.from_iterable(results))\n",
    "  inputs, sparse_inputs = zip(*results)\n",
    "  return (np.asarray(inputs), sparse_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] inputs: (652591, 115, 2, 13) sparse_inputs: (652591, None, 15)\n"
     ]
    }
   ],
   "source": [
    "# Build inputs\n",
    "\n",
    "if maxevents == -1:\n",
    "  zone_hits = RaggedTensorLazyValue(values=zone_hits_l[0], row_splits=zone_hits_l[1])\n",
    "else:\n",
    "  zone_hits = RaggedTensorLazyValue(values=zone_hits_l[0], row_splits=zone_hits_l[1][:(maxevents + 1)])\n",
    "\n",
    "zone_hits_l = None  # release from memory\n",
    "for file_handle in file_handles:  # close files\n",
    "  file_handle.close()\n",
    "\n",
    "inputs, sparse_inputs = build_inputs_all()\n",
    "sparse_inputs_shape = (len(sparse_inputs),) + (None,) + (sparse_inputs[0].shape[-1],)\n",
    "logger.info('inputs: {0} sparse_inputs: {1}'.format(inputs.shape, sparse_inputs_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2,    0,  2548,    5,   18,   17,    6,    6,    0,    4,    4,    0,    0,    0,    1],\n",
      " [  19,    0,  2684,    2,   16,   16,    6,    6,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  28,    0,  2819,   15,   17,   16,    5,    5,    0,    4,    4,    0,    0,    0,    1],\n",
      " [  28,    1,  2728,    0,   16,   17,    5,    5,    0,    4,    4,    0,    0,    0,    1],\n",
      " [  37,    0,  2736,    0,   16,   16,    5,    5,    0,    4,    4,    0,    0,    0,    1],\n",
      " [  55,    0,  2505,    0,   18,   18,    2,    2,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  73,    0,  2675,    0,   19,   19,    2,    2,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  82,    0,  2888,    0,   17,   17,    1,    1,    1,    4,    4,    0,    0,    0,    1],\n",
      " [  82,    1,  2714,    0,   17,   17,    5,    5,    1,    4,    4,    0,    0,    0,    1],\n",
      " [  91,    0,  2737,    0,   17,   17,    1,    1,    0,    4,    4,    0,    0,    0,    1],\n",
      " [ 109,    0,  2479,   15,   17,   17,    6,    6,    0,    4,    4,    1,    0,    0,    1]]\n",
      "[[  29,    0,  4503,    1,   11,   11,    5,    5,    0,    4,    6,    1,    0,   -1,    1],\n",
      " [  11,    0,  4643,    0,   12,   12,    4,    4,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  20,    0,  4528,    0,   11,   11,    5,    5,    0,    4,    4,    0,    0,    0,    1],\n",
      " [  38,    0,  4485,   -2,   11,   11,    6,    6,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  74,    0,  4547,    0,   12,   12,    2,    2,    0,    4,    4,    0,    0,    0,    1],\n",
      " [  83,    0,  4483,    0,   10,   10,    4,    4,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  92,    0,  4483,    0,   11,   11,    2,    2,    1,    4,    4,    1,    0,    0,    1],\n",
      " [ 113,    0,  4661,   -6,   13,   13,    6,    6,    0,    4,    4,    1,    0,    0,    1]]\n",
      "[[   9,    0,  3420,   15,   17,   17,    6,    6,    0,    4,    4,    1,    0,    0,    1],\n",
      " [   9,    1,  3365,  -10,   17,   17,    5,    5,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  19,    0,  2992,   -3,   15,   15,    4,    4,    0,    4,    4,    0,    0,    0,    1],\n",
      " [  28,    0,  2943,    1,   14,   14,    5,    5,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  37,    0,  2956,    2,   14,   14,    5,    5,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  63,    0,  3401,    0,   18,   18,    2,    2,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  82,    0,  2938,    0,   15,   15,    3,    3,   -1,    4,    4,    1,    0,    0,    1],\n",
      " [  91,    0,  2975,    0,   14,   14,    1,    1,   -1,    4,    4,    1,    0,    0,    1],\n",
      " [ 111,    0,  3445,  -28,   18,   18,    6,    6,    0,    4,    4,    1,    0,    0,    1],\n",
      " [  37,    0,  2956,    2,   14,   14,    5,    5,    0,    4,    0,    1,    0,    1,    1]]\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "def tf_constant_value(tensor):\n",
    "  if tf.is_tensor(tensor):\n",
    "    try:\n",
    "      return tensor.numpy()\n",
    "    except:\n",
    "      return tensor\n",
    "  else:\n",
    "    return tensor\n",
    "\n",
    "my_array2string = functools.partial(np.array2string, separator=', ', formatter={'int':lambda x: '% 4i' % x}, max_line_width=100, threshold=1000)\n",
    "\n",
    "print(my_array2string(sparse_inputs[0]))\n",
    "print(my_array2string(sparse_inputs[2]))\n",
    "print(my_array2string(sparse_inputs[5]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Debug\n",
    "for i in range(num_emtf_zones):\n",
    "  for j in range(num_emtf_patterns):\n",
    "    print(i, j, np.array2string(patterns_reshaped[i, j, :, 0], separator=', '))\n",
    "print()\n",
    "\n",
    "for i in range(num_emtf_zones):\n",
    "  for j in range(num_emtf_patterns):\n",
    "    print(i, j, np.array2string(patterns_reshaped[i, j, :, 1], separator=', '))\n",
    "print()\n",
    "\n",
    "for i in range(num_emtf_zones):\n",
    "  for j in range(num_emtf_patterns):\n",
    "    print(i, j, np.array2string(patterns_reshaped[i, j, :, 2], separator=', '))\n",
    "print()\n",
    "\n",
    "for i in range(num_emtf_zones):\n",
    "  paddings = np.abs(patterns_reshaped[i, :, :, 0] - patterns_reshaped[i, 0, 0, 1]).max(axis=0)\n",
    "  print(i, np.array2string(paddings, separator=', '))\n",
    "print()\n",
    "\n",
    "for i in range(num_emtf_zones):\n",
    "  print(i, np.array2string(boxes_qual_reshaped[i], separator=', '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_constants():\n",
    "  # Utility functions & LUTs\n",
    "  inverse_fn = lambda F, y: [[i for (i, y_i) in enumerate(F) if y_i == y_j] for y_j in y]\n",
    "  to_array = lambda x: np.asarray([np.asarray(x_i) for x_i in x])\n",
    "  to_list = lambda x: [x_i.tolist() for x_i in x]\n",
    "  flatten = lambda x: np.asarray([x_i_i for x_i in x for x_i_i in x_i])\n",
    "\n",
    "  def to_array(x):  # improved version\n",
    "    is_ragged = len(set([len(x_i) for x_i in x])) > 1\n",
    "    if is_ragged:\n",
    "      return np.asarray([np.asarray(x_i) for x_i in x], dtype=np.object)\n",
    "    else:\n",
    "      return np.asarray([x_i for x_i in x])\n",
    "\n",
    "  class Constants:\n",
    "    pass\n",
    "\n",
    "  cc = Constants()\n",
    "\n",
    "  cc.site_to_img_row_luts = site_to_img_row_luts\n",
    "\n",
    "  host_to_chamber_lut = to_array(inverse_fn(chamber_to_host_lut, np.arange(num_emtf_hosts)))\n",
    "  #host_to_chamber_lut_flat = flatten(host_to_chamber_lut)\n",
    "\n",
    "  site_to_host_lut = to_array(inverse_fn(host_to_site_lut, np.arange(num_emtf_sites)))\n",
    "  site_to_chamber_lut = to_array([\n",
    "      [c for host in hosts for c in host_to_chamber_lut[host]] \\\n",
    "      for hosts in site_to_host_lut\n",
    "  ])\n",
    "  site_numbers = to_array([\n",
    "      np.repeat(i, len(x)) for i, x in enumerate(site_to_chamber_lut)\n",
    "  ])\n",
    "  cc.site_to_chamber_lut = site_to_chamber_lut\n",
    "  cc.site_numbers = site_numbers\n",
    "\n",
    "  cc.img_row_to_chamber_lut = np.empty(num_emtf_zones, dtype=np.object)\n",
    "  cc.img_row_numbers = np.empty(num_emtf_zones, dtype=np.object)\n",
    "\n",
    "  for i in range(num_emtf_zones):\n",
    "    host_to_img_row_lut = find_emtf_img_row_lut()[:, i]\n",
    "    img_row_to_host_lut = to_array(inverse_fn(host_to_img_row_lut, np.arange(num_rows)))\n",
    "    img_row_to_chamber_lut = to_array([\n",
    "        [c for host in hosts for c in host_to_chamber_lut[host]] \\\n",
    "        for hosts in img_row_to_host_lut\n",
    "    ])\n",
    "    img_row_numbers = to_array([\n",
    "        np.repeat(i, len(x)) for i, x in enumerate(img_row_to_chamber_lut)\n",
    "    ])\n",
    "    cc.img_row_to_chamber_lut[i] = img_row_to_chamber_lut\n",
    "    cc.img_row_numbers[i] = img_row_numbers\n",
    "  return cc\n",
    "\n",
    "cc = prepare_constants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_zone_image(x_ievt, zone, timezone, image_format):\n",
    "  def get_boolean_mask(zone, row):\n",
    "    indices = cc.img_row_to_chamber_lut[zone][row]\n",
    "    boolean_mask = np.zeros(num_emtf_chambers, dtype=np.bool)\n",
    "    boolean_mask[indices] = 1\n",
    "    return boolean_mask\n",
    "\n",
    "  new_ind_emtf_phi = new_hits_metadata['emtf_phi']\n",
    "  new_ind_zones = new_hits_metadata['zones']\n",
    "  new_ind_tzones = new_hits_metadata['tzones']\n",
    "  new_ind_valid = new_hits_metadata['valid']\n",
    "\n",
    "  x_emtf_phi = x_ievt[..., new_ind_emtf_phi]\n",
    "  x_zones = x_ievt[..., new_ind_zones]\n",
    "  x_tzones = x_ievt[..., new_ind_tzones]\n",
    "  x_valid = x_ievt[..., new_ind_valid]\n",
    "\n",
    "  # Valid for this zone\n",
    "  _valid = (x_valid == 1) & \\\n",
    "           (x_zones & (1<<((num_emtf_zones - 1) - zone))).astype(np.bool) & \\\n",
    "           (x_tzones & (1<<((num_emtf_timezones - 1) - timezone))).astype(np.bool)\n",
    "\n",
    "  # Prepare zone image\n",
    "  img = np.zeros(image_format, dtype=np.bool)\n",
    "\n",
    "  # Loop over rows\n",
    "  for row in range(image_format[0]):\n",
    "    boolean_mask = get_boolean_mask(zone, row)\n",
    "    valid = _valid[boolean_mask]\n",
    "    col = find_emtf_img_col(x_emtf_phi[boolean_mask][valid])\n",
    "    row = (col * 0) + row\n",
    "    channel = (col * 0)\n",
    "    img[(row, col, channel)] = 1  # fill\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_track_cand(x_ievt, x, idx_h, idx_w, idx_z, num_features):\n",
    "  def get_phi_patt(zone, patt, row, col, _which):\n",
    "    # In the following, (0, 0, 4) is zone 0 patt 'straightest' row 'ME2/1'\n",
    "    if _which == 'start':\n",
    "      translation = patterns_reshaped[zone, patt, row, 0] - patterns_reshaped[0, 0, 4, 1]\n",
    "    elif _which == 'mid':\n",
    "      translation = patterns_reshaped[zone, patt, row, 1] - patterns_reshaped[0, 0, 4, 1]\n",
    "    elif _which == 'stop':\n",
    "      translation = patterns_reshaped[zone, patt, row, 2] - patterns_reshaped[0, 0, 4, 1]\n",
    "    else:\n",
    "      raise ValueError('Invalid: %s' % _which)\n",
    "    col = col + translation\n",
    "    phi_patt = find_emtf_img_col_inverse(col)\n",
    "    return phi_patt\n",
    "\n",
    "  def get_img_row(zone, site):\n",
    "    return cc.site_to_img_row_luts[zone][site].item()\n",
    "\n",
    "  def get_boolean_mask(zone, site):\n",
    "    indices = cc.site_to_chamber_lut[site]\n",
    "    boolean_mask = np.zeros(num_emtf_chambers, dtype=np.bool)\n",
    "    boolean_mask[indices] = 1\n",
    "    return boolean_mask\n",
    "\n",
    "  new_ind_emtf_phi = new_hits_metadata['emtf_phi']\n",
    "  new_ind_emtf_bend = new_hits_metadata['emtf_bend']\n",
    "  new_ind_emtf_theta1 = new_hits_metadata['emtf_theta1']\n",
    "  new_ind_emtf_theta2 = new_hits_metadata['emtf_theta2']\n",
    "  new_ind_emtf_qual1 = new_hits_metadata['emtf_qual1']\n",
    "  new_ind_emtf_qual2 = new_hits_metadata['emtf_qual2']\n",
    "  new_ind_emtf_time = new_hits_metadata['emtf_time']\n",
    "  new_ind_zones = new_hits_metadata['zones']\n",
    "  new_ind_tzones = new_hits_metadata['tzones']\n",
    "  new_ind_valid = new_hits_metadata['valid']\n",
    "\n",
    "  x_emtf_phi = x_ievt[..., new_ind_emtf_phi]\n",
    "  x_emtf_bend = x_ievt[..., new_ind_emtf_bend]\n",
    "  x_emtf_theta1 = x_ievt[..., new_ind_emtf_theta1]\n",
    "  x_emtf_theta2 = x_ievt[..., new_ind_emtf_theta2]\n",
    "  x_emtf_qual1 = x_ievt[..., new_ind_emtf_qual1]\n",
    "  x_emtf_qual2 = x_ievt[..., new_ind_emtf_qual2]\n",
    "  x_emtf_time = x_ievt[..., new_ind_emtf_time]\n",
    "  x_zones = x_ievt[..., new_ind_zones]\n",
    "  x_tzones = x_ievt[..., new_ind_tzones]\n",
    "  x_valid = x_ievt[..., new_ind_valid]\n",
    "\n",
    "  x_seg = np.arange(num_emtf_chambers * num_emtf_segments).reshape((num_emtf_chambers, num_emtf_segments))\n",
    "\n",
    "  invalid_marker_ph_seg = (num_emtf_chambers * num_emtf_segments)\n",
    "\n",
    "  (zone, patt, col, qual, bx) = (idx_z.item(), idx_h.item(), idx_w.item(), x.item(), 0)\n",
    "\n",
    "  # Prepare track_cand, track_cand_seg\n",
    "  track_cand = np.zeros(num_features, dtype=np.int32)\n",
    "  track_cand_seg = np.zeros(num_emtf_sites, dtype=np.int32) + invalid_marker_ph_seg\n",
    "\n",
    "  num_feature_axes = 7  # (emtf_phi, emtf_bend, emtf_theta1, emtf_theta2, emtf_qual1, emtf_qual2, emtf_time)\n",
    "  feature_values = np.zeros(num_feature_axes * num_emtf_sites, dtype=x_ievt.dtype)\n",
    "\n",
    "  # Valid for this zone\n",
    "  _valid = (x_valid == 1) & \\\n",
    "           (x_zones & (1<<((num_emtf_zones - 1) - zone))).astype(np.bool) & \\\n",
    "           (x_tzones & (1<<((num_emtf_timezones - 1) - timezone))).astype(np.bool)\n",
    "\n",
    "  # Loop over sites\n",
    "  for site in range(num_emtf_sites):\n",
    "    row = get_img_row(zone, site)\n",
    "    phi_patt_start = get_phi_patt(zone, patt, row, col, 'start')\n",
    "    phi_patt_mid = get_phi_patt(zone, patt, row, col, 'mid')\n",
    "    phi_patt_stop = get_phi_patt(zone, patt, row, col, 'stop')\n",
    "\n",
    "    _valid_phi = (find_emtf_img_col(phi_patt_start) <= find_emtf_img_col(x_emtf_phi)) & \\\n",
    "                 (find_emtf_img_col(x_emtf_phi) <= find_emtf_img_col(phi_patt_stop))\n",
    "\n",
    "    boolean_mask = get_boolean_mask(zone, site)\n",
    "    valid = (_valid & _valid_phi)[boolean_mask]\n",
    "    phi_values = x_emtf_phi[boolean_mask][valid]\n",
    "\n",
    "    if len(phi_values):\n",
    "      # Select min dphi\n",
    "      dphi = np.abs(phi_values - phi_patt_mid)\n",
    "      idx = np.argmin(dphi)\n",
    "\n",
    "      # Set segment indices\n",
    "      track_cand_seg[site] = x_seg[boolean_mask][valid][idx]\n",
    "\n",
    "      # Set feature_values\n",
    "      feature_values[np.arange(num_feature_axes) * num_emtf_sites + site] = [\n",
    "          x_emtf_phi[boolean_mask][valid][idx],\n",
    "          x_emtf_bend[boolean_mask][valid][idx],\n",
    "          x_emtf_theta1[boolean_mask][valid][idx],\n",
    "          x_emtf_theta2[boolean_mask][valid][idx],\n",
    "          x_emtf_qual1[boolean_mask][valid][idx],\n",
    "          x_emtf_qual2[boolean_mask][valid][idx],\n",
    "          x_emtf_time[boolean_mask][valid][idx],\n",
    "      ]\n",
    "      # End if len(phi_values)\n",
    "    # End loop over site\n",
    "\n",
    "  # Find phi_median and theta_median\n",
    "  get_ind_emtf_phi_at_site = lambda site: (new_ind_emtf_phi * num_emtf_sites) + site\n",
    "  get_ind_emtf_theta1_at_site = lambda site: (new_ind_emtf_theta1 * num_emtf_sites) + site\n",
    "  get_ind_emtf_theta2_at_site = lambda site: (new_ind_emtf_theta2 * num_emtf_sites) + site\n",
    "\n",
    "  theta_values = feature_values[[\n",
    "      get_ind_emtf_theta1_at_site(2),\n",
    "      get_ind_emtf_theta1_at_site(3),\n",
    "      get_ind_emtf_theta1_at_site(4),\n",
    "      get_ind_emtf_theta2_at_site(2),\n",
    "      get_ind_emtf_theta2_at_site(3),\n",
    "      get_ind_emtf_theta2_at_site(4),\n",
    "      get_ind_emtf_theta1_at_site(6),\n",
    "      get_ind_emtf_theta1_at_site(7),\n",
    "      get_ind_emtf_theta1_at_site(8),\n",
    "  ]]\n",
    "  theta_values_s1 = feature_values[[\n",
    "      get_ind_emtf_theta1_at_site(1),\n",
    "      get_ind_emtf_theta1_at_site(0),\n",
    "      get_ind_emtf_theta1_at_site(11),\n",
    "      get_ind_emtf_theta2_at_site(1),\n",
    "      get_ind_emtf_theta2_at_site(0),\n",
    "      get_ind_emtf_theta2_at_site(11),\n",
    "      get_ind_emtf_theta1_at_site(5),\n",
    "      get_ind_emtf_theta1_at_site(9),\n",
    "      get_ind_emtf_theta1_at_site(11),\n",
    "  ]]\n",
    "\n",
    "  if theta_values[6] == 0:\n",
    "    theta_values[6] = feature_values[get_ind_emtf_theta1_at_site(10)]\n",
    "  theta_values_s1[2] = 0\n",
    "  theta_values_s1[5] = 0\n",
    "\n",
    "  def find_theta_median():\n",
    "    def find_median_of_three(x):\n",
    "      x = np.sort(x)\n",
    "      return pick_the_median(x[x > 0]) if np.any(x > 0) else pick_the_first(x)\n",
    "    #\n",
    "    def find_median_of_nine(x):\n",
    "      fn = find_median_of_three\n",
    "      return fn((fn(x[0:3]), fn(x[3:6]), fn(x[6:9])))\n",
    "    #\n",
    "    if np.any(theta_values > 0):\n",
    "      return find_median_of_nine(theta_values)\n",
    "    else:\n",
    "      return find_median_of_nine(theta_values_s1)\n",
    "\n",
    "  phi_median = find_emtf_img_col_inverse(col)\n",
    "  theta_median = find_theta_median()\n",
    "\n",
    "  # Require theta window, find best theta values\n",
    "  valid_track = False\n",
    "  for site in range(num_emtf_sites):\n",
    "    theta1 = feature_values[get_ind_emtf_theta1_at_site(site)]\n",
    "    theta2 = feature_values[get_ind_emtf_theta2_at_site(site)]\n",
    "    dtheta1 = np.abs(theta1 - theta_median)\n",
    "    dtheta2 = np.abs(theta2 - theta_median)\n",
    "    th_window = 8\n",
    "    valid_site = (theta1 != 0 and theta2 != 0) and (dtheta1 < th_window or dtheta2 < th_window)\n",
    "    valid_track = valid_track or valid_site\n",
    "    if dtheta2 < dtheta1:\n",
    "      feature_values[get_ind_emtf_theta1_at_site(site)] = theta2\n",
    "\n",
    "    feature_values[get_ind_emtf_phi_at_site(site)] -= phi_median\n",
    "    feature_values[get_ind_emtf_theta1_at_site(site)] -= theta_median\n",
    "    if not valid_site:\n",
    "      feature_values[np.arange(num_feature_axes) * num_emtf_sites + site] = ma_fill_value\n",
    "\n",
    "  # Extract features\n",
    "  feature_values_indices = [\n",
    "     0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,  # emtf_phi\n",
    "    24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,  # emtf_theta\n",
    "    12, 13, 14, 15, 16, 23,                          # emtf_bend\n",
    "    48, 49, 50, 51, 52, 59,                          # emtf_qual\n",
    "  ]\n",
    "  phi_median_1 = (phi_median - find_emtf_img_col_inverse(num_cols // 2))\n",
    "  additional_features = [phi_median_1, theta_median, qual, bx]\n",
    "  additional_features = np.array(additional_features, dtype=x_ievt.dtype)\n",
    "  if not valid_track:\n",
    "    additional_features[:] = 0\n",
    "\n",
    "  track_cand[:len(feature_values_indices)] = feature_values[feature_values_indices]\n",
    "  track_cand[len(feature_values_indices):] = additional_features\n",
    "  return (track_cand, track_cand_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_track_dupes(track_cands, track_cands_seg, num_features, num_tracks):\n",
    "  def find_seg_rm(x):\n",
    "    mask = (x != invalid_marker_ph_seg)\n",
    "    return pick_the_first(x[mask]) if np.any(mask) else pick_the_first(x)\n",
    "\n",
    "  invalid_marker_ph_seg = (num_emtf_chambers * num_emtf_segments)\n",
    "\n",
    "  track_cands_seg_indices = np.array([[0, 9, 1, 5], [2, 10, 6], [3, 7], [4, 8], [11]], dtype=np.object)\n",
    "  assert(len(track_cands_seg_indices) == num_emtf_sites_rm)\n",
    "\n",
    "  # Prepare track_cands_rm, track_cands_seg_rm\n",
    "  track_cands_rm = np.zeros(track_cands.shape, dtype=track_cands.dtype)\n",
    "  track_cands_seg_rm = np.zeros(track_cands_seg.shape, dtype=track_cands_seg.dtype) + invalid_marker_ph_seg\n",
    "\n",
    "  # Loop over tracks\n",
    "  for itrk in range(num_tracks):\n",
    "    for site in range(num_emtf_sites_rm):\n",
    "      track_cands_seg_rm[itrk, site] = find_seg_rm(track_cands_seg[itrk, track_cands_seg_indices[site]])\n",
    "\n",
    "  # Mark for removal\n",
    "  removal = np.zeros(num_tracks, dtype=np.bool)\n",
    "  for i in range(num_tracks - 1):\n",
    "    for j in range(i + 1, num_tracks):\n",
    "      removal[j] |= np.any(\n",
    "          (track_cands_seg_rm[i] != invalid_marker_ph_seg) & \\\n",
    "          (track_cands_seg_rm[j] != invalid_marker_ph_seg) & \\\n",
    "          (track_cands_seg_rm[i] == track_cands_seg_rm[j])\n",
    "      )\n",
    "\n",
    "  # Gather survivors\n",
    "  i = 0\n",
    "  for j in range(num_tracks):\n",
    "    if not removal[j]:\n",
    "      track_cands_rm[i] = track_cands[j]\n",
    "      i += 1\n",
    "  if i != num_tracks:\n",
    "    track_cands_rm[i:] = ma_fill_value\n",
    "  return track_cands_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom layers\n",
    "# See: https://www.tensorflow.org/tutorials/customization/custom_layers\n",
    "\n",
    "class Zoning(k_layers.Layer):\n",
    "  def __init__(self, zone, timezone, image_format=image_format, **kwargs):\n",
    "    super(Zoning, self).__init__(**kwargs)\n",
    "    self.zone = zone\n",
    "    self.timezone = timezone\n",
    "    self.image_format = image_format\n",
    "\n",
    "    # Set up py_func\n",
    "    kwargs = dict(zone=self.zone, timezone=self.timezone, image_format=self.image_format)\n",
    "    _build_zone_image = functools.partial(build_zone_image, **kwargs)\n",
    "    #self.py_func = lambda x: tf.py_function(_build_zone_image, [x], tf.bool)\n",
    "    self.py_func = lambda x: tf.numpy_function(_build_zone_image, [x], tf.bool)\n",
    "    self.py_func_1 = lambda x: tf.map_fn(self.py_func, x, fn_output_signature=tf.bool)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x_ievt = inputs\n",
    "\n",
    "    # Call py_func\n",
    "    x = self.py_func_1(x_ievt)\n",
    "    output_shape = (None,) + self.image_format\n",
    "    x.set_shape(output_shape)\n",
    "    return x\n",
    "\n",
    "class Pooling(k_layers.Layer):\n",
    "  def __init__(self, zone, image_format=image_format, num_patterns=num_emtf_patterns, **kwargs):\n",
    "    super(Pooling, self).__init__(**kwargs)\n",
    "    self.zone = zone\n",
    "    self.image_format = image_format\n",
    "    self.num_patterns = num_patterns\n",
    "\n",
    "    # SeparableConv2D but only the depthwise conv (i.e. without the pointwise conv)\n",
    "    # See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D\n",
    "    # See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D\n",
    "    from k_layers_separable_conv2d import SeparableConv2D as MySeparableConv2D\n",
    "    w_init = keras.initializers.Constant(boxes_act_reshaped[self.zone])\n",
    "    conv2d_kwargs = dict(filters=1, kernel_size=(boxes_act_reshaped.shape[1], boxes_act_reshaped.shape[2]), depth_multiplier=self.num_patterns,\n",
    "                         strides=(1, 1), padding='same', activation=None, use_bias=False,\n",
    "                         depthwise_initializer=w_init, pointwise_initializer='ones', trainable=False)\n",
    "    self.conv2d = MySeparableConv2D(**conv2d_kwargs)\n",
    "\n",
    "    # Embedding\n",
    "    # See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "    w_init = keras.initializers.Constant(boxes_qual_reshaped[self.zone])\n",
    "    num_embedding_input_dim = (2 ** self.image_format[0])\n",
    "    embedding_kwargs = dict(input_dim=num_embedding_input_dim, output_dim=1, input_length=1,\n",
    "                            embeddings_initializer=w_init, trainable=False)\n",
    "    self.embedding = k_layers.Embedding(**embedding_kwargs)\n",
    "\n",
    "    # Dot product coeffs for packing the last axis\n",
    "    self.po2_coeffs = (2 ** np.arange(self.image_format[0])).astype(np.int32)  # [1,2,4,8,16,32,64,128]\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Conv\n",
    "    x = inputs  # NHWC, which is (None, 8, 288, 1)\n",
    "    x = tf.cast(x, dtype=tf.float32)\n",
    "    x = tf.transpose(x, perm=(0, 3, 2, 1))  # NHWC -> NCWH\n",
    "    x = self.conv2d(x)  # NCWH -> NCWH', H' is dim of size H * D, D is depth_multipler\n",
    "    x = tf.reshape(x, [-1, self.image_format[2], self.image_format[1], self.image_format[0], self.num_patterns])  # NCWH' -> NCWHD\n",
    "    x = tf.transpose(x, perm=(0, 1, 2, 4, 3))  # NCWHD -> NCWDH\n",
    "    x = tf.reduce_sum(x, axis=1)  # NCWDH -> NWDH, C is dim of size 1 and has been dropped\n",
    "\n",
    "    # Pack 8 bits into a single number\n",
    "    x = tf.clip_by_value(x, 0, 1)\n",
    "    x = tf.cast(x, dtype=tf.int32)\n",
    "    x = tf.reduce_sum(x * self.po2_coeffs, axis=-1)  # NWDH -> NWD, H has been packed into a single number and dropped\n",
    "    x = tf.cast(x, dtype=tf.float32)\n",
    "\n",
    "    # Embedding\n",
    "    x = self.embedding(x)  # NWD -> NWDE, E is embedding output dim\n",
    "    x = tf.reduce_sum(x, axis=-1)  # NWDE -> NWD, E is dim of size 1 and has been dropped\n",
    "    x = tf.cast(x, dtype=tf.int32)\n",
    "\n",
    "    # Gather max element\n",
    "    idx_h = tf.argmax(x, axis=-1, output_type=tf.int32)  # NWD -> NW\n",
    "    x = tf.gather(x, idx_h, axis=-1, batch_dims=2)  # NWD -> NW\n",
    "    return (x, idx_h)\n",
    "\n",
    "class Suppression(k_layers.Layer):\n",
    "  def __init__(self, zone, image_format=image_format, **kwargs):\n",
    "    super(Suppression, self).__init__(**kwargs)\n",
    "    self.zone = zone\n",
    "    self.image_format = image_format\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x, idx_h = inputs\n",
    "\n",
    "    # Non-max suppression\n",
    "    x_padded = tf.pad(x, paddings=((0, 0), (1, 1)))  # ((pad_t, pad_b), (pad_l, pad_r))\n",
    "    mask = (x > x_padded[:, :-2]) & (x >= x_padded[:, 2:])  # x > x_left && x >= x_right\n",
    "    mask = tf.cast(mask, dtype=x.dtype)\n",
    "    x = x * mask\n",
    "    return (x, idx_h)\n",
    "\n",
    "class ZoneSorting(k_layers.Layer):\n",
    "  def __init__(self, zone, num_tracks=num_emtf_tracks, **kwargs):\n",
    "    super(ZoneSorting, self).__init__(**kwargs)\n",
    "    self.zone = zone\n",
    "    self.num_tracks = num_tracks\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x, idx_h = inputs\n",
    "\n",
    "    # Sort (descending)\n",
    "    idx_w = tf.argsort(x, axis=-1, direction='DESCENDING', stable=True)\n",
    "    idx_w.set_shape(x.shape)\n",
    "    idx_w = tf.transpose(tf.transpose(idx_w)[:self.num_tracks])  # truncate\n",
    "\n",
    "    # Gather max elements\n",
    "    x = tf.gather(x, idx_w, axis=-1, batch_dims=1)  # NW -> NW', W' is dim of size num_tracks\n",
    "    idx_h = tf.gather(idx_h, idx_w, axis=-1, batch_dims=1)  # NW -> NW'\n",
    "    return (x, idx_h, idx_w)\n",
    "\n",
    "class ZoneMerging(k_layers.Layer):\n",
    "  def __init__(self, num_tracks=num_emtf_tracks, **kwargs):\n",
    "    super(ZoneMerging, self).__init__(**kwargs)\n",
    "    self.num_tracks = num_tracks\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x, idx_h, idx_w = inputs\n",
    "\n",
    "    # Sort (descending)\n",
    "    idx_z = tf.argsort(x, axis=-1, direction='DESCENDING', stable=True)\n",
    "    idx_z.set_shape(x.shape)\n",
    "    idx_z = tf.transpose(tf.transpose(idx_z)[:self.num_tracks])  # truncate\n",
    "\n",
    "    # Gather max elements\n",
    "    x = tf.gather(x, idx_z, axis=-1, batch_dims=1)  # NW' -> NW\", W\" is dim of size num_tracks\n",
    "    idx_h = tf.gather(idx_h, idx_z, axis=-1, batch_dims=1)  # NW' -> NW\"\n",
    "    idx_w = tf.gather(idx_w, idx_z, axis=-1, batch_dims=1)  # NW' -> NW\"\n",
    "    idx_z = idx_z // self.num_tracks\n",
    "    return (x, idx_h, idx_w, idx_z)\n",
    "\n",
    "class TrkBuilding(k_layers.Layer):\n",
    "  def __init__(self, num_features=num_emtf_features, num_tracks=num_emtf_tracks, **kwargs):\n",
    "    super(TrkBuilding, self).__init__(**kwargs)\n",
    "    self.num_features = num_features\n",
    "    self.num_tracks = num_tracks\n",
    "\n",
    "    # Set up py_func\n",
    "    kwargs = dict(num_features=self.num_features)\n",
    "    _build_track_cand = functools.partial(build_track_cand, **kwargs)\n",
    "    #self.py_func = lambda x: tf.py_function(_build_track_cand, x, (tf.int32, tf.int32))\n",
    "    self.py_func = lambda x: tf.numpy_function(_build_track_cand, x, (tf.int32, tf.int32))\n",
    "    self.py_func_1 = lambda x: tf.map_fn(self.py_func, x, fn_output_signature=[tf.int32, tf.int32])\n",
    "    self.py_func_2 = lambda x: tf.map_fn(self.py_func_1, x, fn_output_signature=[tf.int32, tf.int32])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x_ievt, x, idx_h, idx_w, idx_z = inputs\n",
    "\n",
    "    # Call py_func\n",
    "    x_ievt = tf.stack([x_ievt for _ in range(self.num_tracks)], axis=1)\n",
    "    x = (x_ievt, x, idx_h, idx_w, idx_z)\n",
    "    x = self.py_func_2(x)\n",
    "    output_shape = (None,) + (self.num_tracks, self.num_features)\n",
    "    x[0].set_shape(output_shape)\n",
    "    output_shape = (None,) + (self.num_tracks, num_emtf_sites)\n",
    "    x[1].set_shape(output_shape)\n",
    "    return x\n",
    "\n",
    "class DupeRemoval(k_layers.Layer):\n",
    "  def __init__(self, num_features=num_emtf_features, num_tracks=num_emtf_tracks, **kwargs):\n",
    "    super(DupeRemoval, self).__init__(**kwargs)\n",
    "    self.num_features = num_features\n",
    "    self.num_tracks = num_tracks\n",
    "\n",
    "    # Set up py_func\n",
    "    kwargs = dict(num_features=self.num_features, num_tracks=self.num_tracks)\n",
    "    _remove_track_dupes = functools.partial(remove_track_dupes, **kwargs)\n",
    "    #self.py_func = lambda x: tf.py_function(_remove_track_dupes, x, tf.int32)\n",
    "    self.py_func = lambda x: tf.numpy_function(_remove_track_dupes, x, tf.int32)\n",
    "    self.py_func_1 = lambda x: tf.map_fn(self.py_func, x, fn_output_signature=tf.int32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = inputs\n",
    "\n",
    "    # Call py_func\n",
    "    x = self.py_func_1(x)\n",
    "    output_shape = (None,) + (self.num_tracks, self.num_features)\n",
    "    x.set_shape(output_shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"awesome_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 115, 2, 13)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zoning_0 (Zoning)               (None, 8, 288, 1)    0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "zoning_1 (Zoning)               (None, 8, 288, 1)    0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "zoning_2 (Zoning)               (None, 8, 288, 1)    0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pooling_0 (Pooling)             ((None, 288), (None, 6528        zoning_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooling_1 (Pooling)             ((None, 288), (None, 6528        zoning_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooling_2 (Pooling)             ((None, 288), (None, 6528        zoning_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "suppression_0 (Suppression)     ((None, 288), (None, 0           pooling_0[0][0]                  \n",
      "                                                                 pooling_0[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "suppression_1 (Suppression)     ((None, 288), (None, 0           pooling_1[0][0]                  \n",
      "                                                                 pooling_1[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "suppression_2 (Suppression)     ((None, 288), (None, 0           pooling_2[0][0]                  \n",
      "                                                                 pooling_2[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "zonesorting_0 (ZoneSorting)     ((None, 4), (None, 4 0           suppression_0[0][0]              \n",
      "                                                                 suppression_0[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "zonesorting_1 (ZoneSorting)     ((None, 4), (None, 4 0           suppression_1[0][0]              \n",
      "                                                                 suppression_1[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "zonesorting_2 (ZoneSorting)     ((None, 4), (None, 4 0           suppression_2[0][0]              \n",
      "                                                                 suppression_2[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 12)           0           zonesorting_0[0][0]              \n",
      "                                                                 zonesorting_1[0][0]              \n",
      "                                                                 zonesorting_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12)           0           zonesorting_0[0][1]              \n",
      "                                                                 zonesorting_1[0][1]              \n",
      "                                                                 zonesorting_2[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 12)           0           zonesorting_0[0][2]              \n",
      "                                                                 zonesorting_1[0][2]              \n",
      "                                                                 zonesorting_2[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "zonemerging_0 (ZoneMerging)     ((None, 4), (None, 4 0           concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "trkbuilding_0 (TrkBuilding)     [(None, 4, 40), (Non 0           inputs[0][0]                     \n",
      "                                                                 zonemerging_0[0][0]              \n",
      "                                                                 zonemerging_0[0][1]              \n",
      "                                                                 zonemerging_0[0][2]              \n",
      "                                                                 zonemerging_0[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "duperemoval_0 (DupeRemoval)     (None, 4, 40)        0           trkbuilding_0[0][0]              \n",
      "                                                                 trkbuilding_0[0][1]              \n",
      "==================================================================================================\n",
      "Total params: 19,584\n",
      "Trainable params: 0\n",
      "Non-trainable params: 19,584\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "  # Input\n",
    "  inputs = keras.Input(shape=(num_emtf_chambers, num_emtf_segments, num_emtf_variables), dtype='int32', name='inputs')\n",
    "  x = inputs\n",
    "\n",
    "  # Loop over zones\n",
    "  x_list = []\n",
    "\n",
    "  for i in range(num_emtf_zones):\n",
    "    # Make zone images\n",
    "    x_i = Zoning(zone=i, timezone=timezone, name='zoning_{0}'.format(i))(x)\n",
    "\n",
    "    # Pattern recognition\n",
    "    x_i = Pooling(zone=i, name='pooling_{0}'.format(i))(x_i)\n",
    "    x_i = Suppression(zone=i, name='suppression_{0}'.format(i))(x_i)\n",
    "\n",
    "    # Sort zone outputs\n",
    "    x_i = ZoneSorting(zone=i, name='zonesorting_{0}'.format(i))(x_i)\n",
    "\n",
    "    # Add x_i to x_list\n",
    "    x_list.append(x_i)\n",
    "\n",
    "  # Merge zone outputs\n",
    "  i = next(iter(range(num_emtf_zones)))\n",
    "  x = (k_layers.Concatenate(axis=-1)([x[0] for x in x_list]),\n",
    "       k_layers.Concatenate(axis=-1)([x[1] for x in x_list]),\n",
    "       k_layers.Concatenate(axis=-1)([x[2] for x in x_list]))\n",
    "  x = ZoneMerging(name='zonemerging_{0}'.format(i))(x)\n",
    "\n",
    "  # Track builder\n",
    "  x = (inputs,) + x\n",
    "  x = TrkBuilding(name='trkbuilding_{0}'.format(i))(x)\n",
    "  x = DupeRemoval(name='duperemoval_{0}'.format(i))(x)\n",
    "\n",
    "  # Output\n",
    "  outputs = x\n",
    "\n",
    "  # Model\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs, name='awesome_model')\n",
    "  model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "  # Summary\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "  keras.utils.plot_model(model, to_file=model.name+'.dot', show_shapes=True)\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "!dot -Teps awesome_model.dot > awesome_model.eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "  def __init__(self, x, batch_size=None, steps=None):\n",
    "    self.x = x\n",
    "    self.num_samples = int(x.shape[0])\n",
    "    if not batch_size:\n",
    "      batch_size = int(np.ceil(self.num_samples / float(steps))) if steps else 32\n",
    "    self.batch_size = batch_size\n",
    "    self.num_batches = int(np.ceil(self.num_samples / float(batch_size)))\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_batches\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    #print('Processing batch {0}'.format(index))\n",
    "    start, stop = (index * self.batch_size, min(self.num_samples, (index + 1) * self.batch_size))\n",
    "    return self.x[start:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] outputs: (652591, 4, 40) type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "\n",
    "batch_size = 10000\n",
    "outputs = model.predict(DataGenerator(inputs, batch_size=batch_size), workers=workers, use_multiprocessing=False)  # now wait...\n",
    "logger.info('outputs: {0} type: {1}'.format(outputs.shape, type(outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Wrote to features_zone0.h5\n"
     ]
    }
   ],
   "source": [
    "# Write to file\n",
    "if maxevents == -1:\n",
    "  outfile = 'features_zone{0}.h5'.format(zone)\n",
    "  outdict = {'features': da.from_array(outputs[:, 0, :]), 'truths': zone_part_l}\n",
    "  da.to_hdf5(outfile, outdict, compression='lzf')\n",
    "  logger.info('Wrote to {0}'.format(outfile))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Debug\n",
    "x = tf_constant_value(outputs)\n",
    "x[x == ma_fill_value] = 0\n",
    "\n",
    "for ievt in range(len(x)):\n",
    "  print(my_array2string(sparse_inputs[ievt]))\n",
    "print()\n",
    "\n",
    "for ievt in range(len(x)):\n",
    "  print(my_array2string(x[ievt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoning_0_out:\n",
      "(array([127]), array([0]))\n",
      "(array([132]), array([0]))\n",
      "(array([143, 149]), array([0, 0]))\n",
      "pooling_0_out:\n",
      "(array([ 93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n",
      "       111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
      "       129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,\n",
      "       147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]),)\n",
      "(array([230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "       266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "       284, 285, 286, 287]),)\n",
      "(array([148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
      "       166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
      "       202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "       220, 221, 222]),)\n",
      "suppression_0_out:\n",
      "(array([ 93,  97, 106, 123, 129, 133, 139]),) [18 26 38 38 38 38 63]\n",
      "(array([230, 237, 244, 248, 251, 255]),) [18 33 34 36 48 63]\n",
      "(array([149, 154, 157, 159]),) [ 3 32 55 63]\n",
      "zonesorting_0_out:\n",
      "(array([0, 1, 2, 3]),) [63 38 38 38]\n",
      "(array([0, 1, 2, 3]),) [63 48 36 34]\n",
      "(array([0, 1, 2, 3]),) [63 55 32  3]\n",
      "zonemerging_0_out:\n",
      "(array([0, 1, 2, 3]),) [63 38 38 38]\n",
      "(array([0, 1, 2, 3]),) [63 48 36 34]\n",
      "(array([0, 1, 2, 3]),) [63 55 32  3]\n",
      "trkbuilding_0_out:\n",
      "[[-116    0   20   64   72    0    0   50   73 -159   11 -185    1    0    0    0    0    0    0\n",
      "     1    1    2    3    1    5    0    2    0    0   15    6    0    6    5    5    6  -80   16\n",
      "    63    0]\n",
      " [ 412    0    0    0    0    0    0    0    0  369    0  343    0    0    0    0    0    0    0\n",
      "     0    0    1    0    0    5    0    0    0    0   15    6    0    0    0    0    6 -608   17\n",
      "    38    0]\n",
      " [ 140    0    0    0    0    0    0    0    0   97    0   71    0    0    0    0    0    0    0\n",
      "     0    0    1    0    0    5    0    0    0    0   15    6    0    0    0    0    6 -336   17\n",
      "    38    0]\n",
      " [  44    0    0    0    0    0    0    0    0    1    0  -25    0    0    0    0    0    0    0\n",
      "     0    0    1    0    0    5    0    0    0    0   15    6    0    0    0    0    6 -240   17\n",
      "    38    0]]\n",
      "[[ 123    0    8  -17  -35    0    0  -37  -37    0   27  141    1    0    0    0    0    0    0\n",
      "    -1    0    0    1    2    0    0    0    1   -2   -6    4    0    5    5    6    6 1776   11\n",
      "    63    0]\n",
      " [ 187    0    0   47   29    0    0   27   27    0    0  205    1    0    0    0    0    0    0\n",
      "    -1    0    0    0    2    0    0    0    1   -2   -6    4    0    0    5    6    6 1712   11\n",
      "    48    0]\n",
      " [ 235    0    0    0   77    0    0   75   75    0    0  253    1    0    0    0    0    0    0\n",
      "    -1    0    0    0    2    0    0    0    0   -2   -6    4    0    0    0    6    6 1664   11\n",
      "    36    0]\n",
      " [ 299    0    0    0  141    0    0    0  139    0    0  317    1    0    0    0    0    0    0\n",
      "     0    0    0    0    2    0    0    0    0   -2   -6    4    0    0    0    6    6 1600   11\n",
      "    34    0]]\n",
      "[[381   0   8 -41   0   0   0 -46  -9 417   0 461   3   0   1   0   0   0   0   1   0   4   0   4\n",
      "  -10   0  -3   1   0 -28   5   0   4   5   0   6 240  14  63   0]\n",
      " [413   0   0  -9   0   0   0 -14  23 449   0 493   3   0   0   0   0   0   0   1   0   4   0   4\n",
      "  -10   0   0   1   0 -28   5   0   0   5   0   6 208  14  55   0]\n",
      " [  0   0   0  39   0   0   0  34  71 497   0 541   0   0   0   0   0   0   0   1   0   4   0   4\n",
      "    0   0   0   1   0 -28   0   0   0   5   0   6 160  14  32   0]\n",
      " [  0   0   0 119   0   0   0 114 151   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "    0   0   0   1   0   0   0   0   0   5   0   0  80  14   3   0]]\n",
      "duperemoval_0_out:\n",
      "[[-116    0   20   64   72    0    0   50   73 -159   11 -185    1    0    0    0    0    0    0\n",
      "     1    1    2    3    1    5    0    2    0    0   15    6    0    6    5    5    6  -80   16\n",
      "    63    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "[[ 123    0    8  -17  -35    0    0  -37  -37    0   27  141    1    0    0    0    0    0    0\n",
      "    -1    0    0    1    2    0    0    0    1   -2   -6    4    0    5    5    6    6 1776   11\n",
      "    63    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "[[381   0   8 -41   0   0   0 -46  -9 417   0 461   3   0   1   0   0   0   0   1   0   4   0   4\n",
      "  -10   0  -3   1   0 -28   5   0   4   5   0   6 240  14  63   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "tiny_inputs = inputs[:10]\n",
    "model_zoning_0 = keras.Model(inputs=model.input, outputs=model.get_layer('zoning_0').output)\n",
    "model_pooling_0 = keras.Model(inputs=model.input, outputs=model.get_layer('pooling_0').output)\n",
    "model_suppression_0 = keras.Model(inputs=model.input, outputs=model.get_layer('suppression_0').output)\n",
    "model_zonesorting_0 = keras.Model(inputs=model.input, outputs=model.get_layer('zonesorting_0').output)\n",
    "model_zonemerging_0 = keras.Model(inputs=model.input, outputs=model.get_layer('zonemerging_0').output)\n",
    "model_trkbuilding_0 = keras.Model(inputs=model.input, outputs=model.get_layer('trkbuilding_0').output)\n",
    "model_duperemoval_0 = keras.Model(inputs=model.input, outputs=model.get_layer('duperemoval_0').output)\n",
    "\n",
    "outputs = model_zoning_0(tiny_inputs)\n",
    "x = tf_constant_value(outputs[0])\n",
    "with np.printoptions(linewidth=100, threshold=1000):\n",
    "  print('zoning_0_out:')\n",
    "  print(x[0].nonzero())\n",
    "  print(x[2].nonzero())\n",
    "  print(x[5].nonzero())\n",
    "\n",
    "outputs = model_pooling_0(tiny_inputs)\n",
    "x = tf_constant_value(outputs[0])\n",
    "with np.printoptions(linewidth=100, threshold=1000):\n",
    "  print('pooling_0_out:')\n",
    "  print(x[0].nonzero())\n",
    "  print(x[2].nonzero())\n",
    "  print(x[5].nonzero())\n",
    "\n",
    "outputs = model_suppression_0(tiny_inputs)\n",
    "x = tf_constant_value(outputs[0])\n",
    "with np.printoptions(linewidth=100, threshold=1000):\n",
    "  print('suppression_0_out:')\n",
    "  print(x[0].nonzero(), x[0][x[0].nonzero()])\n",
    "  print(x[2].nonzero(), x[2][x[2].nonzero()])\n",
    "  print(x[5].nonzero(), x[5][x[5].nonzero()])\n",
    "\n",
    "outputs = model_zonesorting_0(tiny_inputs)\n",
    "x = tf_constant_value(outputs[0])\n",
    "with np.printoptions(linewidth=100, threshold=1000):\n",
    "  print('zonesorting_0_out:')\n",
    "  print(x[0].nonzero(), x[0][x[0].nonzero()])\n",
    "  print(x[2].nonzero(), x[2][x[2].nonzero()])\n",
    "  print(x[5].nonzero(), x[5][x[5].nonzero()])\n",
    "\n",
    "outputs = model_zonemerging_0(tiny_inputs)\n",
    "x = tf_constant_value(outputs[0])\n",
    "with np.printoptions(linewidth=100, threshold=1000):\n",
    "  print('zonemerging_0_out:')\n",
    "  print(x[0].nonzero(), x[0][x[0].nonzero()])\n",
    "  print(x[2].nonzero(), x[2][x[2].nonzero()])\n",
    "  print(x[5].nonzero(), x[5][x[5].nonzero()])\n",
    "\n",
    "outputs = model_trkbuilding_0(tiny_inputs)\n",
    "x = tf_constant_value(outputs[0])\n",
    "x[x == ma_fill_value] = 0\n",
    "with np.printoptions(linewidth=100, threshold=1000):\n",
    "  print('trkbuilding_0_out:')\n",
    "  print(x[0])\n",
    "  print(x[2])\n",
    "  print(x[5])\n",
    "\n",
    "outputs = model_duperemoval_0(tiny_inputs)\n",
    "x = tf_constant_value(outputs)\n",
    "x[x == ma_fill_value] = 0\n",
    "with np.printoptions(linewidth=100, threshold=1000):\n",
    "  print('duperemoval_0_out:')\n",
    "  print(x[0])\n",
    "  print(x[2])\n",
    "  print(x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
